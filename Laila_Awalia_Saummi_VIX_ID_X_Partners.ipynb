{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lailasaummi/Virtual-Internship-id-x-partners/blob/main/Laila_Awalia_Saummi_VIX_ID_X_Partners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Credit Risk Prediction for a Lending Company**\n",
        "\n",
        "Company provided the dataset that contains of good and bad credit scoring issued from 2007 until 2014.\n",
        "\n",
        "Objectives:\n",
        "1.   Built a model with provided technology solution to predict the probability of a borrower defaulting a loan\n",
        "2.   Developed WOE, IV and Train-Test to get the best model\n",
        "3.   Deployed the Machine Learning model using pickle\n",
        "\n"
      ],
      "metadata": {
        "id": "QhwRxB2Kkgaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Resources and References\n",
        "\n",
        "*   Loan Dataset 2007-2014 (format .csv) https://drive.google.com/file/d/1r17UjbuxkcCwGbXOUkr3wcG8UmjvEzCD/view?usp=sharing\n",
        "*   Loan Dataset Dictionary (sheet: LoanStats) https://docs.google.com/spreadsheets/d/1iT1JNOBwU4l616_rnJpo0iny7blZvNBs/edit#gid=1001272030\n",
        "*   Credit Risk Modelling in Python https://medium.com/analytics-vidhya/credit-risk-modelling-in-python-3ab4b00f6505\n",
        "*   Credit Risk Modelling Git https://github.com/yineme/Credit_Risk_modelling.git\n",
        "*   Uji Multikolinearitas pada Analisis Regresi https://lab_adrk.ub.ac.id/id/uji-multikolinearitas-pada-analisis-regresi/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K6fWWnNszdcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation**"
      ],
      "metadata": {
        "id": "rUfMIZcW_ygD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries to Process the Dataset"
      ],
      "metadata": {
        "id": "GFklE8Fw3zyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "j_4KT4Kk7G0u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset Dictionary\n",
        "Understand the representation of each column in dataset"
      ],
      "metadata": {
        "id": "6m8QPGef4AL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kxOkh9qj5Z9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Description = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/LCDataDictionary.xlsx', 'LoanStats').dropna()\n",
        "Description.style.set_properties(subset=['Description'], **{'width' :'850px'})"
      ],
      "metadata": {
        "id": "H70P2K7639m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFqDHXnkWcnb"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/loan_data_2007_2014.csv', low_memory = False)"
      ],
      "metadata": {
        "id": "Lgrqb3qQ6_UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dimension of dataset\n",
        "data.shape"
      ],
      "metadata": {
        "id": "ZLNwKGUI9Idl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KRG0EL4MNRJ"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Inconsistent Columns"
      ],
      "metadata": {
        "id": "4o06OqGplF5n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC3US_mRBUhf"
      },
      "source": [
        "``Unnamed: 0``, ``id``, ``member_id``, ``url``, ``title``, ``zip_code``, ``emp_title`` and ``policy_code`` grade columns are considered as identifiers and can not be used in this modelling. The ``sub_grade`` columns also contains same information as grade columns. \n",
        "\n",
        "Columns that contains of future information like ``next_pymnt_d``, ``recoveries``, ``collection_recovery_fee``, ``total_rec_prncp`` and ``total_rec_late_fee`` grade columns can not be used because those events aren't yet occur. The sub_grade columns also contains same information as grade columns. Then we can drop them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijjrqvIzBUhh"
      },
      "outputs": [],
      "source": [
        "data.drop(['Unnamed: 0','id','member_id', 'sub_grade', 'url', 'title','zip_code', 'emp_title', 'policy_code'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "4DtM3FrgC-Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['next_pymnt_d', 'recoveries', 'collection_recovery_fee', 'total_rec_prncp', 'total_rec_late_fee'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "_ARCsqngH8D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "bsQOqZh8IGyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filling Columns Contain of Missing Values"
      ],
      "metadata": {
        "id": "RfI0iB3UnuOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHfXvvYhMNR7"
      },
      "outputs": [],
      "source": [
        "# Expand the output display of columns that have missing values\n",
        "pd.options.display.max_rows = None\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slUZbBL6MNR-"
      },
      "source": [
        "Loan Dataset Dictionary tell us about the description of ``total_rev_hi_lim`` is total revolving high credit/ credit limit. ``total_rev_hi_lim`` have missing values but we need it for credit risk prediction modelling analysis. We can recover it with ``funded_amnt`` column (The total amount committed to that loan at that point in time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlNEFsGlMNSB"
      },
      "outputs": [],
      "source": [
        "data['total_rev_hi_lim'].fillna(data['funded_amnt'],inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_yOVTQ9MNSC"
      },
      "outputs": [],
      "source": [
        "data['total_rev_hi_lim'].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loan Dataset Dictionary tell us about the description of ``annual_inc`` is The self-reported annual income provided by the borrower during registration. ``annual_inc`` have missing values but we need it for credit risk prediction modelling analysis. We can recover it with fill the missing values with ``annual_inc`` mean values."
      ],
      "metadata": {
        "id": "KrYK6laOkZ4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uux3R01ZMNSI"
      },
      "outputs": [],
      "source": [
        "data.annual_inc.fillna(data.annual_inc.mean(),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "897DJIjbMNSH"
      },
      "outputs": [],
      "source": [
        "data.annual_inc.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNPxOXtuMNSJ"
      },
      "source": [
        "We can consider the other columns with missing values that we need for credit risk prediction modelling with fill them by zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHiLz3sUMNSK"
      },
      "outputs": [],
      "source": [
        "for i in list(['acc_now_delinq','total_acc','pub_rec','open_acc','inq_last_6mths','delinq_2yrs','emp_length']):\n",
        "    data[i].fillna(0,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ALV65p3MNSQ"
      },
      "outputs": [],
      "source": [
        "data.loc[:,['acc_now_delinq','total_acc','pub_rec','open_acc','inq_last_6mths','delinq_2yrs','emp_length']].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Columns Contain of Missing Values"
      ],
      "metadata": {
        "id": "N8qweE6wn6f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display columns that have greater than 70% of missing values\n",
        "missing_values = data.isnull().mean()\n",
        "missing_values[missing_values>0.7]"
      ],
      "metadata": {
        "id": "V0vrLUTvd1RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = ['desc', 'mths_since_last_record', 'mths_since_last_major_derog', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m']"
      ],
      "metadata": {
        "id": "ifTWqjF_CF0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=missing_values, inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "hah3OtztCZYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "ZicARSn8FDQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DWmVZ0zbtlV"
      },
      "source": [
        "Create a new variable called ``good_bad`` will help us for scoring the loan whether is good or bad loan\n",
        "\n",
        "good --> 1<br>\n",
        "bad --> 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column based on 'loan_status' column that will be our target variable\n",
        "data['good_bad'] = np.where(data.loc[:, 'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)',\n",
        "                                                                       'Does not meet the credit policy. Status:Charged Off']), 0, 1)\n",
        "# Drop the original 'loan_status' column\n",
        "data.drop(columns = ['loan_status'], inplace = True)"
      ],
      "metadata": {
        "id": "3_mwF3ygbza2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "sj00Afrbb8D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "ewCteGrkb35E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Plotting Libraries"
      ],
      "metadata": {
        "id": "w64BWMM2IadU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BiAx1AQtIhYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Multicollinear Features from Dataset"
      ],
      "metadata": {
        "id": "6AiuV7c_exV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is multicollinearity in dataset, then the predictive power of a variable that is strongly correlated with other variables not reliable and unstable. So we should drop them."
      ],
      "metadata": {
        "id": "FFW1Qpl0e_VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop another missing values column\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "INH-dnv0hHl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "jKv41JLPhhzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHpB3R5oBUi5"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix showing correlation co-effiecients \n",
        "corr_matrix = data.corr()\n",
        "heatMap=sns.heatmap(corr_matrix, annot=True,  cmap=\"BrBG\", annot_kws={'size':12})\n",
        "heatmap=plt.gcf()\n",
        "heatmap.set_size_inches(20,15)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CT3kHCDBUi7"
      },
      "outputs": [],
      "source": [
        "# Drop multicollinear features \n",
        "data.drop(columns=['loan_amnt', 'revol_bal', 'funded_amnt', 'funded_amnt_inv', 'installment',  'total_pymnt_inv',  'out_prncp_inv',  'total_acc'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZ_RBXgBUi8"
      },
      "outputs": [],
      "source": [
        "corr_matrix = data.corr()\n",
        "heatMap=sns.heatmap(corr_matrix, annot=True,  cmap=\"BrBG\", annot_kws={'size':12})\n",
        "heatmap=plt.gcf()\n",
        "heatmap.set_size_inches(20,15)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAsAJHfMBUjA"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "-G00tZdFhOg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "lHKuUtSyhUB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yx_-bamMNRK"
      },
      "source": [
        "### Converting Data Types of Continous Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugc6UOkNMNRR"
      },
      "outputs": [],
      "source": [
        "# Display unique value of 'emp_length' column\n",
        "data['emp_length'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FanpNJfMNRW"
      },
      "outputs": [],
      "source": [
        "# Convert 'emp_length' datatype to numerical column and assign missing values to zero\n",
        "def emp_length_convert(df, column):\n",
        "    df[column] = df[column].str.replace('\\+ years', '')\n",
        "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
        "    df[column] = df[column].str.replace(' years', '')\n",
        "    df[column] = df[column].str.replace(' year', '')\n",
        "    df[column] = pd.to_numeric(df[column])\n",
        "    df[column].fillna(value = 0, inplace = True)\n",
        "    \n",
        "emp_length_convert(data, 'emp_length')\n",
        "\n",
        "data['emp_length'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['emp_length'].dtype"
      ],
      "metadata": {
        "id": "ci1d7ownffOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'term' datatype to numerical column\n",
        "def term_numeric(df, column):\n",
        "    df[column] = pd.to_numeric(df[column].str.replace(' months', ''))\n",
        "    \n",
        "term_numeric(data, 'term')"
      ],
      "metadata": {
        "id": "6R1MDy5Pkmmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['term'].dtype"
      ],
      "metadata": {
        "id": "vI4FQEtmkbWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified date columns \n",
        "\n",
        "def date_columns(df, column):\n",
        "    # Current month\n",
        "    today_date = pd.to_datetime('2020-08-01')\n",
        "    # Convert to datetime format\n",
        "    df[column] = pd.to_datetime(df[column], format = \"%b-%y\")\n",
        "    # Calculate the difference in months and add to a new column\n",
        "    df['mths_since_' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))\n",
        "    # Make any resulting -ve values to be equal to the max date\n",
        "    df['mths_since_' + column] = df['mths_since_' + column].apply(lambda x: df['mths_since_' + column].max() if x < 0 else x)\n",
        "    # Drop the original date column\n",
        "    df.drop(columns = [column], inplace = True)\n",
        "    \n",
        "\n",
        "date_columns(data, 'issue_d')\n",
        "date_columns(data, 'last_pymnt_d')\n",
        "date_columns(data, 'last_credit_pull_d')\n",
        "date_columns(data, 'earliest_cr_line')"
      ],
      "metadata": {
        "id": "4N-a7KpSlUX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FioRfgncMNRu"
      },
      "source": [
        "### Concatenate Discrete Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-KlWibyMNRv"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiRJouQMMNRw"
      },
      "outputs": [],
      "source": [
        "# Create dummy variables for categorical columns\n",
        "pd.get_dummies(data['grade'],prefix = 'grade', prefix_sep = \":\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO362FOrMNRx"
      },
      "outputs": [],
      "source": [
        "data_dummies = [pd.get_dummies(data['grade'],prefix = 'grade', prefix_sep = \":\"),\n",
        "                    pd.get_dummies(data['home_ownership'],prefix = 'home_ownership', prefix_sep = \":\"),\n",
        "                    pd.get_dummies(data['verification_status'],prefix = 'verification_status', prefix_sep = \":\"),\n",
        "                    pd.get_dummies(data['good_bad'],prefix = 'good_bad', prefix_sep = \":\"),\n",
        "                    pd.get_dummies(data['purpose'],prefix = 'purpose', prefix_sep = \":\"),\n",
        "                    pd.get_dummies(data['addr_state'],prefix = 'addr_state', prefix_sep = \":\"),\n",
        "                    pd.get_dummies(data['initial_list_status'],prefix = 'initial_list_status', prefix_sep = \":\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yGnCJeoMNRz"
      },
      "outputs": [],
      "source": [
        "data_dummies = pd.concat(data_dummies,axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3PfYEcwMNR3"
      },
      "outputs": [],
      "source": [
        "type(data_dummies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ8CJTdHMNR4"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([data,data_dummies],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb4PHz-KMNR5"
      },
      "outputs": [],
      "source": [
        "data.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRURW1RYBUkP"
      },
      "outputs": [],
      "source": [
        "# Check for missing values columns again \n",
        "missing_values = data.isnull().sum()\n",
        "missing_values[missing_values>0]/len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P3bvN_zqwkq"
      },
      "outputs": [],
      "source": [
        "preprocess_data = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgSfCXeaqwkq"
      },
      "outputs": [],
      "source": [
        "# Check for any missing values\n",
        "missing = preprocess_data.isnull().sum()\n",
        "missing[missing>0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Supervised Learning**"
      ],
      "metadata": {
        "id": "3G0lqovAtvM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight of Evidence (WOE) and Information Value (IV)"
      ],
      "metadata": {
        "id": "HhWmX50TuJFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight of evidence (WOE) can determine which categories should be binned. Information value (IV) can determine which variables are useful in the logistic regression which is the algorithm of supervised learning. "
      ],
      "metadata": {
        "id": "XE_eTFK2uoRM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrfY-SJ_qwku"
      },
      "outputs": [],
      "source": [
        "# Calculate WOE and IV\n",
        "\n",
        "def iv_woe(data, target, bins=10, show_woe=False):\n",
        "    \n",
        "    # Empty Dataframe\n",
        "    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n",
        "    \n",
        "    # Extract Column Names\n",
        "    cols = data.columns\n",
        "    \n",
        "    # Run WOE and IV on all the independent variables\n",
        "    for ivars in cols[~cols.isin([target])]:\n",
        "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
        "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
        "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
        "        else:\n",
        "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
        "        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n",
        "        d.columns = ['Cutoff', 'N', 'Events']\n",
        "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
        "        d['Non-Events'] = d['N'] - d['Events']\n",
        "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
        "        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n",
        "        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n",
        "        d.insert(loc=0, column='Variable', value=ivars)\n",
        "        print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n",
        "        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n",
        "        newDF=pd.concat([newDF,temp], axis=0)\n",
        "        woeDF=pd.concat([woeDF,d], axis=0)\n",
        "\n",
        "        # Show WOE Table\n",
        "        if show_woe == True:\n",
        "            print(d)\n",
        "    return newDF, woeDF\n",
        "iv, woe = iv_woe(preprocess_data, target='good_bad', bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rNmneneqwkw"
      },
      "outputs": [],
      "source": [
        "print(iv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9GbEEyqIlMV"
      },
      "source": [
        "The rule of dumb is for variables with less than 0.2 of information value are not useful for prediction and if greater than 0.5 have a suspicious predictive power. \n",
        "\n",
        "Therefore, the following variables will not be included: \n",
        "``out_prncp``, ``last_pymnt_amnt``, ``delinq_2yrs``, ``mths_since_last_delinq``, ``open_acc``, ``pub_rec``, ``total_acc``, ``collections_12_mths_ex_med``, ``acc_now_delinq``, ``tot_coll_amt`` and ``mths_since_last_pymnt_d``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkMApgvnqwky"
      },
      "outputs": [],
      "source": [
        "# Drop columns with low information value\n",
        "preprocess_data.drop(columns=[ 'pymnt_plan', 'last_pymnt_amnt', 'revol_util', 'delinq_2yrs', 'mths_since_last_delinq', 'open_acc', 'pub_rec',  'collections_12_mths_ex_med', 'acc_now_delinq',\n",
        "                              'tot_coll_amt', 'mths_since_last_pymnt_d', 'emp_length', 'application_type'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjCj7-zVqwky"
      },
      "outputs": [],
      "source": [
        "preprocess_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emZnDx5Lqwkz"
      },
      "outputs": [],
      "source": [
        "# Create dummy variables for categorical columns\n",
        "data_dummies1 = [pd.get_dummies(preprocess_data['grade'], prefix='grade', prefix_sep=':'),\n",
        "               pd.get_dummies(preprocess_data['home_ownership'], prefix='home_ownership', prefix_sep=':'),\n",
        "               pd.get_dummies(preprocess_data['verification_status'], prefix='verification_status', prefix_sep=':'),\n",
        "                pd.get_dummies(preprocess_data['purpose'], prefix='purpose', prefix_sep=':'),\n",
        "                pd.get_dummies(preprocess_data['addr_state'], prefix='addr_state', prefix_sep=':'),\n",
        "                pd.get_dummies(preprocess_data['initial_list_status'], prefix='initial_list_status', prefix_sep=':')\n",
        "                               \n",
        "               ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al1tbl0Zqwk0"
      },
      "outputs": [],
      "source": [
        "# Turn 'data_dummies' into dataframe\n",
        "\n",
        "categorical_dummies = pd.concat(data_dummies1, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQikVZZdqwk1"
      },
      "outputs": [],
      "source": [
        "categorical_dummies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjbuXVVUqwk1"
      },
      "outputs": [],
      "source": [
        "# Concatinate preprocess_data variable with categorical_dummies\n",
        "\n",
        "preprocess_data = pd.concat([preprocess_data, categorical_dummies], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLLOU2-Qqwk2"
      },
      "outputs": [],
      "source": [
        "preprocess_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmQBr4fLqwk3"
      },
      "outputs": [],
      "source": [
        "preprocess_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5eurlaSqwk4"
      },
      "outputs": [],
      "source": [
        "# Calculate WOE of categorical features\n",
        "\n",
        "def woe_categorical(df, cat_feature, good_bad_df):\n",
        "    df = pd.concat([df[cat_feature], good_bad_df], axis=1)\n",
        "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
        "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
        "    df = df.iloc[:, [0, 1, 3]]\n",
        "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
        "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
        "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
        "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
        "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
        "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
        "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
        "    df = df.sort_values(['WoE'])\n",
        "    df = df.reset_index(drop = True)\n",
        "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
        "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
        "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
        "    df['IV'] = df['IV'].sum()\n",
        "    return df\n",
        "\n",
        "\n",
        "# Plot WOE values \n",
        "# Set seaborn as default style of graphs\n",
        "sns.set()\n",
        "# Plot WoE across categories that takes 2 arguments: a dataframe and a number.\n",
        "def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n",
        "    x = np.array(df_WoE.iloc[:, 0].apply(str))\n",
        "    y = df_WoE['WoE']\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n",
        "    plt.xlabel(df_WoE.columns[0])\n",
        "    plt.ylabel('Weight of Evidence')\n",
        "    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n",
        "    plt.xticks(rotation = rotation_of_x_axis_labels)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRWApiEbqwk6"
      },
      "outputs": [],
      "source": [
        "# Separate data into target and features\n",
        "X= preprocess_data.drop(columns='good_bad', axis=1)\n",
        "y=preprocess_data['good_bad']\n",
        "df_grade = woe_categorical(X, 'grade', y)\n",
        "df_grade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHjZSi8sqwk-"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_grade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXnn5h8Aqwk_"
      },
      "source": [
        "From this graph we can see that grades variable have different WOE from another variable. We will keep each grade as a feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzQJJMsxqwlA"
      },
      "outputs": [],
      "source": [
        "# Analyze 'home_ownership' variable\n",
        "\n",
        "df_home = woe_categorical(X, 'home_ownership', y)\n",
        "df_home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiCTHy1BqwlB"
      },
      "outputs": [],
      "source": [
        "# Plot df_home WOE\n",
        "plot_by_woe(df_home)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaYplUJGqwlC"
      },
      "source": [
        "OTHER, NONE and ANY have very few observations and should be combined with the category with high risk of default which is RENT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POceP_7LqwlD"
      },
      "outputs": [],
      "source": [
        "# Analyze 'verification_status'\n",
        "\n",
        "veri_df = woe_categorical(X, 'verification_status', y)\n",
        "veri_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAFhVVIyqwlE"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(veri_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IALADdvmqwlE"
      },
      "source": [
        "This variable has different WOE values and can be used to separate variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMKiTfa8qwlF"
      },
      "outputs": [],
      "source": [
        "# Analyze 'purpose'  variable\n",
        "pur_df = woe_categorical(X, 'purpose', y)\n",
        "pur_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hpq2zOMqwlG"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(pur_df, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFh8KCV9qwlH"
      },
      "source": [
        "The following  categories wil be combined together:\n",
        "1. educational, renewable_energy, moving\n",
        "2. other,house, medical\n",
        "3. weeding, vacation\n",
        "4. debt_consolidation\n",
        "5. home_improvement, major purchase\n",
        "6. car, credit_card\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSSCdF4sqwlH"
      },
      "outputs": [],
      "source": [
        "# Analyze 'addr_state' WOE\n",
        "\n",
        "addr_df = woe_categorical(X, 'addr_state', y)\n",
        "addr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7NXSJZ8qwlI"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(addr_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OTcZGkCqwlK"
      },
      "source": [
        "The states NE, IA, ME and ID have low observations and this may because of their extreme WOE. We will plot the graph again without including these categories and see if there are any changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhXVMzrIqwlL"
      },
      "outputs": [],
      "source": [
        "# Dataframe excluding low observations for 'addr_state' column\n",
        "data1 =addr_df.iloc[2:44, :]\n",
        "data2 =addr_df.iloc[45:49, :]\n",
        "low_data_woe = pd.concat([data1, data2], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8rROhYtqwlM"
      },
      "outputs": [],
      "source": [
        "low_data_woe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPO2Z-oeqwlQ"
      },
      "outputs": [],
      "source": [
        "# Plot 'addr_state' excluding states with low observations\n",
        "plot_by_woe(low_data_woe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhuA-1urqwlR"
      },
      "source": [
        "To decide which categories will combined, we use both WOE and the number of observations. Categories with similar WOE but significantly different observations will not be combine together. It is because the number of observations could influence the WOE values. Also, categories with similar WOE and observations greater than 5% can be combined together to form a new category. \n",
        "\n",
        "The categories will combined together, such as:\n",
        "\n",
        "1. NE, IA, NV, HI, FL, AL\n",
        "2. NY\n",
        "3. LA, NM, OK, NC, MO, MD, NJ, VA\n",
        "4. CA\n",
        "5. AZ, MI, UT, TN, AR, PA\n",
        "6. RI, OH, KY, DE, MN, SD, MA, IN\n",
        "7. GA, WA\n",
        "8. WI, OR\n",
        "9. TX\n",
        "10. IL, CT,MT\n",
        "11. CO, SC\n",
        "12. KS, VT, AK, MS\n",
        "13. NH, WV, WY, DC\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCMlIgRKqwlS"
      },
      "outputs": [],
      "source": [
        "# Analyze 'initial_list_status' WOE \n",
        "\n",
        "init_list_df = woe_categorical( X, 'initial_list_status', y)\n",
        "init_list_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ldABW9CqwlT"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(init_list_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1kw8kjfqwlU"
      },
      "source": [
        "This variable has significantly different WOE values and categories should be kept as separate variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDQuVuNtqwlV"
      },
      "source": [
        "### Analyze Continous Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac7M5tYXqwlW"
      },
      "outputs": [],
      "source": [
        "# Function to calculate WOE for continous variables\n",
        "def woe_continous(df, cat_feature, good_bad_df):\n",
        "    df = pd.concat([df[cat_feature], good_bad_df], axis=1)\n",
        "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
        "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
        "    df = df.iloc[:, [0, 1, 3]]\n",
        "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
        "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
        "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
        "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
        "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
        "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
        "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
        "#     df = df.sort_values(['WoE'])\n",
        "#     df = df.reset_index(drop = True)\n",
        "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
        "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
        "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
        "    df['IV'] = df['IV'].sum()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwEIl3nqqwlX"
      },
      "outputs": [],
      "source": [
        "# Analyze 'term' WOE\n",
        "plot_by_woe(woe_continous(X,'term', y ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNBNmK_NqwlY"
      },
      "outputs": [],
      "source": [
        "X['mths_since_issue_d'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIRQoA8jqwlZ"
      },
      "outputs": [],
      "source": [
        "# Fine classing by create a new variable\n",
        "\n",
        "X['mths_since_issue_d_factor'] = pd.cut(X['mths_since_issue_d'], 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVvbGs48qwla"
      },
      "outputs": [],
      "source": [
        "# Analyze 'mths_since_iss_df' WOE\n",
        "mths_since_iss_df = woe_continous(X, 'mths_since_issue_d_factor', y)\n",
        "mths_since_iss_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OHsYDLndqwld"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(mths_since_iss_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuCKg55qqwlf"
      },
      "source": [
        "The following categories will be created based on their WOE and number of observations:\n",
        "1. (67.97, 70.8)\n",
        "2. (70.8, 73.6)\n",
        "3. (73.6- 76.4)\n",
        "4. (76.4.- 79.2)\n",
        "5. (79.2-82)\n",
        "6. 82-84\n",
        "7. 84-90.4\n",
        "8. 90.4-96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhzOm2USqwlf"
      },
      "outputs": [],
      "source": [
        "# Analyze interest rate WOE\n",
        "X['int_rate_factor'] = pd.cut(X['int_rate'], 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPaLbD-nqwlg"
      },
      "outputs": [],
      "source": [
        "int_rate_df = woe_continous(X, 'int_rate_factor',y)\n",
        "int_rate_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-haQmeSqwlh"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(int_rate_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8gL7cu-qwli"
      },
      "source": [
        "This graph shows that only the last two categories will be combined. That is:\n",
        "1. (22.048, 26) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65UZAZ2aqwli"
      },
      "outputs": [],
      "source": [
        "# Analyze 'tot_rec_int' WOE\n",
        "X['total_rec_int_factor'] = pd.cut(X['total_rec_int'], 20)\n",
        "rec_int_df = woe_continous(X, 'total_rec_int_factor', y)\n",
        "rec_int_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojmat7Qmqwlj"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(rec_int_df, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeENd6fDqwlk"
      },
      "outputs": [],
      "source": [
        "# Analyze 'total_revol_hi_lim' WOE\n",
        "X['total_rev_hi_lim_factor'] = pd.cut(X['total_rev_hi_lim'], 100)\n",
        "revol_hi_df = woe_continous(X, 'total_rev_hi_lim_factor', y)\n",
        "revol_hi_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7umSE0dGqwll"
      },
      "outputs": [],
      "source": [
        "# Analyze income below 100000\n",
        "# Analyze income below 150000\n",
        "X_train_prepr_temp = X[X['total_rev_hi_lim'] <= 100000].copy()\n",
        "# Fine-classing again\n",
        "X_train_prepr_temp['total_rev_hi_lim_factor'] = pd.cut(X_train_prepr_temp['total_rev_hi_lim'],10)\n",
        "# Select only the relevant indexes in the target column\n",
        "df_temp = woe_continous(X_train_prepr_temp, 'total_rev_hi_lim_factor', y[X_train_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy4amv2Zqwlm"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnz-sZZzqwln"
      },
      "outputs": [],
      "source": [
        "# Analyze 'total_pymnt' WOE\n",
        "X['total_pymnt_factor'] = pd.cut(X['total_pymnt'], 10)\n",
        "total_pym_df = woe_continous(X, 'total_pymnt_factor', y)\n",
        "total_pym_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi_TvrYbqwln"
      },
      "outputs": [],
      "source": [
        "# Analyze 'dti' WOE\n",
        "X['dti_factor'] = pd.cut(X['dti'], 10)\n",
        "dti_df = woe_continous(X, 'dti_factor', y)\n",
        "dti_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktVX8V3qwlo"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(dti_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLG3yWCZqwlp"
      },
      "source": [
        "The following categories will  be combined together:\n",
        "1. (27.993, 31.992), (31.992, 35.991), (35.991, 39.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqpeoRtBqwlq"
      },
      "outputs": [],
      "source": [
        "# Analyze annual income WOE\n",
        "X['annual_inc_factor'] = pd.cut(X['annual_inc'], 50)\n",
        "ann_inc_df = woe_continous(X, 'annual_inc_factor', y)\n",
        "ann_inc_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0VVf3Mkqwls"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(ann_inc_df, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZM_Er5Nqwlu"
      },
      "source": [
        "Separate this variable into person with higher and lower income. From the WOE table we see that as annual income increases, the the number of observations decreases. It is because only a few person earn high income. We will analyze a new variable of person with income above 150000 dollars and below 150000 dollars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ttjSrCpqwlw"
      },
      "outputs": [],
      "source": [
        "# Analyze income below 150000\n",
        "X_train_prepr_temp = X[X['annual_inc'] <= 150000].copy()\n",
        "# Fine-classing again\n",
        "X_train_prepr_temp['annual_inc_factor'] = pd.cut(X_train_prepr_temp['annual_inc'], 10)\n",
        "# Select only the relevant indexes in the target column\n",
        "df_temp = woe_continous(X_train_prepr_temp, 'annual_inc_factor', y[X_train_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyPka7j9qwlz"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xufykdaqqwl3"
      },
      "source": [
        "We will combine the following categories based on WOE and number of observations as follows: \n",
        " (<=32000), (>32000 <= 50000), (>50000 <= 60000), (>60000 <=75000), (>75000 <=90000), (>90000 <=120000), (>120000 <=135000), (>135000 <=150000), (>150000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0EhRmdqwl4"
      },
      "outputs": [],
      "source": [
        "# Analyze 'inq_last_6mths' WOE\n",
        "X['inq_last_6mths_factor'] = pd.cut(X['inq_last_6mths'], 7)\n",
        "inq_fact_df = woe_continous(X, 'inq_last_6mths_factor', y)\n",
        "inq_fact_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SeWc76qwl5"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(inq_fact_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0fSS2B_qwl6"
      },
      "source": [
        "The following categories will be created to new categories:\n",
        "1. <1 months\n",
        "2. 1-2\n",
        "3. 2-4\n",
        "4. 4-7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnphFjw2qwl6"
      },
      "outputs": [],
      "source": [
        "# Analyze total current balance WOE\n",
        "X['tot_cur_bal_factor'] = pd.cut(X['tot_cur_bal'], 20)\n",
        "curr_bal_df = woe_continous(X, 'tot_cur_bal_factor', y)\n",
        "curr_bal_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hElx3pqqwl-"
      },
      "outputs": [],
      "source": [
        "# Analyze total current balance below WOE\n",
        "X_train_prepr_temp = X[X['tot_cur_bal'] <= 400000].copy()\n",
        "# Fine-classing again\n",
        "X_train_prepr_temp['tot_cur_bal_factor'] = pd.cut(X_train_prepr_temp['tot_cur_bal'], 10)\n",
        "# Select only the relevant indexes in the target column\n",
        "df_temp = woe_continous(X_train_prepr_temp, 'tot_cur_bal_factor', y[X_train_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spG-1MlGqwl_"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MNOCpaPqwl_"
      },
      "source": [
        "The variables will be created: \n",
        "<40000\n",
        "40000-80000\n",
        "80000-120000\n",
        "120000-160000\n",
        "160000-200000\n",
        "200000-240000\n",
        "240000-320000\n",
        "320000-400000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVPGTIQ3qwmA"
      },
      "outputs": [],
      "source": [
        "# Analyze 'mths_since_credit_pull' WOE \n",
        "X['mths_since_last_credit_pull_d_factor'] = pd.cut(X['mths_since_last_credit_pull_d'], 10)\n",
        "mths_cr_pull_df = woe_continous(X, 'mths_since_last_credit_pull_d_factor', y)\n",
        "mths_cr_pull_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i8DbEgdqwmB"
      },
      "outputs": [],
      "source": [
        "# Analyze 'mths_since_credit_pull' below 60\n",
        "X_train_prepr_temp = X[X['mths_since_last_credit_pull_d'] <= 60].copy()\n",
        "# Fine-classing again\n",
        "X_train_prepr_temp['mths_since_last_credit_pull_d'] = pd.cut(X_train_prepr_temp['mths_since_last_credit_pull_d'], 5)\n",
        "# Select only the relevant indexes in the target column\n",
        "df_temp = woe_continous(X_train_prepr_temp, 'mths_since_last_credit_pull_d', y[X_train_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI5PuGh9qwmC"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(mths_cr_pull_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Ifa_5IqwmD"
      },
      "source": [
        "The following categories will be grouped together: 54-65, 65-76, greater than 76\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uPwbth8qwmE"
      },
      "outputs": [],
      "source": [
        "# Analyze 'out_prncp' WOE \n",
        "X['out_prncp_factor'] = pd.cut(X['out_prncp'], 10)\n",
        "out_df = woe_continous(X, 'out_prncp_factor', y)\n",
        "out_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NJrrf5QqwmF"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(out_df, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlWh_URfqwmG"
      },
      "outputs": [],
      "source": [
        "# Analyze 'mths_since_issue_date' WOE \n",
        "X['mths_since_issue_d'] = pd.cut(X['mths_since_issue_d'], 10)\n",
        "iss_df = woe_continous(X, 'mths_since_issue_d', y)\n",
        "iss_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeLJlDQjqwmG"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(iss_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw9rVb8TqwmH"
      },
      "source": [
        "### Creating  new features based on WOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yhkeoLeqwmI"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe and start with 'grade' variable\n",
        "\n",
        "new_df = preprocess_data.loc[:, 'grade:A':'grade:G']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRDTscU6qwmJ"
      },
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcilufeHqwmJ"
      },
      "outputs": [],
      "source": [
        "# home_ownership \n",
        "\n",
        "new_df['home_ownership:OWN'] = preprocess_data.loc[:, 'home_ownership:OWN']\n",
        "new_df['home_ownership:OTHER_NONE_RENT_ANY'] = sum([preprocess_data['home_ownership:OTHER'], preprocess_data['home_ownership:NONE'],\n",
        "                                                 preprocess_data['home_ownership:RENT'], preprocess_data['home_ownership:ANY']])\n",
        "new_df['home_ownership:MORTGAGE'] = preprocess_data.loc[:, 'home_ownership:MORTGAGE']\n",
        "\n",
        "# verification_status\n",
        "new_df['verification_status:NOT_VERIFIED'] = preprocess_data.loc[:, 'verification_status:Not Verified']\n",
        "new_df['verification_status:SOURCE_VERIFIED'] = preprocess_data.loc[:, 'verification_status:Source Verified']\n",
        "new_df['verification_status:VERIFIED'] = preprocess_data.loc[:, 'verification_status:Verified']\n",
        "\n",
        "# purpose of loan\n",
        "new_df['purpose:SMALL_BUSINESS_EDUCATIONAL_RENEWABLE_ENERGY_MOVING'] = sum([preprocess_data['purpose:small_business'],  preprocess_data['purpose:renewable_energy'], preprocess_data['purpose:moving']])\n",
        "\n",
        "new_df['purpose:OTHER_HOUSE_MEDICAL'] =sum([preprocess_data['purpose:other'], preprocess_data['purpose:house'], preprocess_data['purpose:medical']])\n",
        "new_df ['purpose:WEDDING_VACATION'] = sum([preprocess_data['purpose:wedding'], preprocess_data['purpose:vacation']])\n",
        "new_df ['purpose:HOME_IMPROVEMENT_MAJOR_PURCHASE'] = sum([preprocess_data['purpose:home_improvement'], preprocess_data['purpose:major_purchase']])\n",
        "new_df ['purpose:CAR_CREDIT_CARD'] = sum([preprocess_data['purpose:car'], preprocess_data['purpose:credit_card']])\n",
        "\n",
        "\n",
        "# addr_state\n",
        "new_df['addr_state:NE_IA_NV_HI_FL_AL'] =sum([preprocess_data['addr_state:IA'],preprocess_data['addr_state:NV'],\n",
        "                                           preprocess_data['addr_state:HI'],preprocess_data['addr_state:FL'],preprocess_data['addr_state:AL']])\n",
        "new_df['addr_state:NY'] = preprocess_data.loc[:, 'addr_state:NY']\n",
        "new_df['addr_state:LA_NM_OK_NC_MO_MD_NJ_VA'] = sum([preprocess_data['addr_state:LA'],preprocess_data['addr_state:NM'],preprocess_data['addr_state:OK'],\n",
        "                     preprocess_data['addr_state:NC'],preprocess_data['addr_state:MO'],preprocess_data['addr_state:MD'], preprocess_data['addr_state:NJ'],\n",
        "                                                  preprocess_data['addr_state:VA']])\n",
        "new_df['addr_state:CA'] = preprocess_data.loc[:,'addr_state:CA']\n",
        "new_df['addr_state:AZ_MI_UT_TN_AR_PA'] =sum([preprocess_data['addr_state:AZ'],preprocess_data['addr_state:MI'],preprocess_data['addr_state:UT'],\n",
        "preprocess_data['addr_state:TN'],preprocess_data['addr_state:AR'],preprocess_data['addr_state:PA']])\n",
        "\n",
        "new_df['addr_state:RI_OH_KY_DE_MN_SD_MA_IN'] =sum([preprocess_data['addr_state:RI'],preprocess_data['addr_state:OH'],preprocess_data['addr_state:KY'],\n",
        " preprocess_data['addr_state:DE'],preprocess_data['addr_state:MN'],preprocess_data['addr_state:SD'],preprocess_data['addr_state:MA'],\n",
        "                    preprocess_data['addr_state:IN']])\n",
        "\n",
        "new_df['addr_state:GA_WA'] = sum([preprocess_data['addr_state:GA'], preprocess_data['addr_state:WA']])\n",
        "new_df['addr_state:WI_OR'] = sum([preprocess_data['addr_state:WI'], preprocess_data['addr_state:OR']])\n",
        "new_df['addr_state:TX'] = preprocess_data.loc[:,'addr_state:TX']\n",
        "new_df['addr_state:IL_CT_MT'] =sum([preprocess_data['addr_state:IL'],preprocess_data['addr_state:CT'],preprocess_data['addr_state:MT']])\n",
        "new_df['addr_state:CO_SC'] = sum([preprocess_data['addr_state:CO'], preprocess_data['addr_state:SC']])\n",
        "new_df['addr_state:KS_VT_AK_NS'] =sum([preprocess_data['addr_state:KS'],preprocess_data['addr_state:VT'],preprocess_data['addr_state:AK'],\n",
        "                                           preprocess_data['addr_state:MS']])\n",
        "new_df['addr_state:NH_WV_WY_DC'] =sum([preprocess_data['addr_state:NH'],preprocess_data['addr_state:WV'],preprocess_data['addr_state:WY'],\n",
        "                                           preprocess_data['addr_state:DC']])\n",
        "# initial_list_status\n",
        "new_df['initial_list_status:F'] = preprocess_data.loc[:, 'initial_list_status:f']\n",
        "new_df['initial_list_status:W'] = preprocess_data.loc[:, 'initial_list_status:w']\n",
        "\n",
        "# term \n",
        "new_df['term:36'] = np.where((preprocess_data['term'] == 36), 1, 0)\n",
        "new_df['term:60'] = np.where((preprocess_data['term']==60), 1,0)\n",
        "\n",
        "# total_rec_int \n",
        "new_df['total_rec_int:<1000'] = np.where((preprocess_data['total_rec_int']<=1000), 1,0)\n",
        "new_df['total_rec_int:1000-2000'] = np.where((preprocess_data['total_rec_int']>1000) &(preprocess_data['total_rec_int']<=2000), 1,0)\n",
        "new_df['total_rec_int:2000-9000'] = np.where((preprocess_data['total_rec_int']>2000) &(preprocess_data['total_rec_int']<=9000), 1,0)\n",
        "new_df['total_rec_int:>9000'] = np.where((preprocess_data['total_rec_int']>9000), 1,0)\n",
        "\n",
        "\n",
        "# total_revol_hi_lim\n",
        "new_df['total_rev_hi_lim:<10000'] =np.where((preprocess_data['total_rev_hi_lim']<=10000),1,0)\n",
        "new_df['total_rev_hi_lim:10000-20000'] =np.where((preprocess_data['total_rev_hi_lim']>10000)&(preprocess_data['total_rev_hi_lim']<=20000),1,0)\n",
        "new_df['total_rev_hi_lim:20000-40000'] =np.where((preprocess_data['total_rev_hi_lim']>20000)&(preprocess_data['total_rev_hi_lim']<=40000),1,0)\n",
        "new_df['total_rev_hi_lim:40000-60000'] =np.where((preprocess_data['total_rev_hi_lim']>40000)&(preprocess_data['total_rev_hi_lim']<=60000),1,0)\n",
        "new_df['total_rev_hi_lim:60000-80000'] =np.where((preprocess_data['total_rev_hi_lim']>60000)&(preprocess_data['total_rev_hi_lim']<=80000),1,0)\n",
        "new_df['total_rev_hi_lim:80000-100000'] =np.where((preprocess_data['total_rev_hi_lim']>80000)&(preprocess_data['total_rev_hi_lim']<=100000),1,0)\n",
        "new_df['total_rev_hi_lim:<100000'] =np.where((preprocess_data['total_rev_hi_lim']>100000),1,0)\n",
        "\n",
        "\n",
        "# total_pymnt\n",
        "new_df['total_pymnt:<5000'] = np.where((preprocess_data['total_pymnt']<=5000), 1,0)\n",
        "new_df['total_pymnt:5000-11000'] = np.where((preprocess_data['total_pymnt']>5000)&(preprocess_data['total_pymnt']<=11000),1,0)\n",
        "new_df['total_pymnt:11000-16000'] = np.where((preprocess_data['total_pymnt']>11000)&(preprocess_data['total_pymnt']<=16000),1,0)\n",
        "new_df['total_pymnt:16000-22000'] = np.where((preprocess_data['total_pymnt']>16000)&(preprocess_data['total_pymnt']<=22000),1,0)\n",
        "new_df['total_pymnt:>22000'] = np.where((preprocess_data['total_pymnt']<=5000), 1,0)\n",
        "#int_Rate\n",
        "\n",
        "new_df['int_rate:<7.484'] = np.where((preprocess_data['int_rate'] <= 7.484), 1, 0)\n",
        "new_df['int_rate:7.484-9.548'] = np.where((preprocess_data['int_rate'] > 7.484) & (preprocess_data['int_rate'] <= 9.548), 1, 0)\n",
        "new_df['int_rate:9.548-11.612'] = np.where((preprocess_data['int_rate'] > 9.548) & (preprocess_data['int_rate'] <= 11.612), 1, 0)\n",
        "new_df['int_rate:11.612-13.676'] = np.where((preprocess_data['int_rate'] > 11.612) & (preprocess_data['int_rate'] <= 13.676), 1, 0)\n",
        "new_df['int_rate:13.676-15.74'] = np.where((preprocess_data['int_rate'] > 13.676) & (preprocess_data['int_rate'] <= 15.74), 1, 0)\n",
        "new_df['int_rate:15.74-17.804'] = np.where((preprocess_data['int_rate'] > 15.74) & (preprocess_data['int_rate'] <= 17.804), 1, 0)\n",
        "new_df['int_rate:17.804-19.868'] = np.where((preprocess_data['int_rate'] > 17.804) & (preprocess_data['int_rate'] <= 19.868), 1, 0)\n",
        "new_df['int_rate:7.19.868-21.932'] = np.where((preprocess_data['int_rate'] > 19.868) & (preprocess_data['int_rate'] <= 21.932), 1, 0)\n",
        "new_df['int_rate:21.932-26.06'] = np.where((preprocess_data['int_rate'] > 21.932) & (preprocess_data['int_rate'] <= 26.06), 1, 0)\n",
        "\n",
        "\n",
        "# dti \n",
        "new_df['dti:<4'] = np.where((preprocess_data['dti'] <=4), 1, 0)\n",
        "new_df['dti:4-8'] = np.where((preprocess_data['dti'] > 4) & (preprocess_data['dti'] <= 8), 1, 0)\n",
        "new_df['dti:8-12'] = np.where((preprocess_data['dti'] > 8) & (preprocess_data['dti'] <= 12), 1, 0)\n",
        "new_df['dti:12-16'] = np.where((preprocess_data['dti'] > 12) & (preprocess_data['dti'] <= 16), 1, 0)\n",
        "new_df['dti:16-20'] = np.where((preprocess_data['dti'] > 16) & (preprocess_data['dti'] <= 20), 1, 0)\n",
        "new_df['dti:20-23'] = np.where((preprocess_data['dti'] > 20) & (preprocess_data['dti'] <= 23), 1, 0)\n",
        "new_df['dti:23-27'] = np.where((preprocess_data['dti'] > 23) & (preprocess_data['dti'] <= 27), 1, 0)\n",
        "new_df['dti:27-40'] = np.where((preprocess_data['dti'] > 27) & (preprocess_data['dti'] <= 40), 1, 0)\n",
        "\n",
        "# annual income \n",
        "new_df['annual_inc:<32000'] = np.where((preprocess_data['annual_inc'] <= 32000), 1, 0)\n",
        "new_df['annual_inc:32000-50000'] = np.where((preprocess_data['annual_inc'] > 32000) & (preprocess_data['annual_inc'] <= 50000),1, 0)\n",
        "new_df['annual_inc:32000-50000'] = np.where((preprocess_data['annual_inc'] > 32000) & (preprocess_data['annual_inc'] <= 50000), 1, 0)\n",
        "new_df['annual_inc:50000-60000'] = np.where((preprocess_data['annual_inc'] > 50000) & (preprocess_data['annual_inc'] <= 60000), 1, 0)\n",
        "new_df['annual_inc:60000-75000'] = np.where((preprocess_data['annual_inc'] > 60000) & (preprocess_data['annual_inc'] <= 75000), 1, 0)\n",
        "new_df['annual_inc:75000-90000'] = np.where((preprocess_data['annual_inc'] > 75000) & (preprocess_data['annual_inc'] <= 90000), 1, 0)\n",
        "new_df['annual_inc:90000-120000'] = np.where((preprocess_data['annual_inc'] > 90000) & (preprocess_data['annual_inc'] <= 120000), 1, 0)\n",
        "new_df['annual_inc:120000-135000'] = np.where((preprocess_data['annual_inc'] > 120000) & (preprocess_data['annual_inc'] <= 135000), 1, 0)\n",
        "new_df['annual_inc:135000-150000'] = np.where((preprocess_data['annual_inc'] > 135000) & (preprocess_data['annual_inc'] <= 150000), 1, 0)\n",
        "new_df['annual_inc:>150000'] = np.where((preprocess_data['annual_inc'] > 150000), 1, 0)\n",
        "\n",
        "# inq_last_6mths\n",
        "new_df['inq_last_6mths:<1'] = np.where((preprocess_data['inq_last_6mths'] <=1), 1, 0)\n",
        "new_df['inq_last_6mths:1-2'] = np.where((preprocess_data['inq_last_6mths'] >1)& (preprocess_data['inq_last_6mths']<=2),  1, 0)\n",
        "new_df['inq_last_6mths:2-4'] = np.where((preprocess_data['inq_last_6mths'] >2)& (preprocess_data['inq_last_6mths']<=4),  1, 0)\n",
        "new_df['inq_last_6mths:4-7'] = np.where((preprocess_data['inq_last_6mths'] >4)& (preprocess_data['inq_last_6mths']<=7),  1, 0)\n",
        "\n",
        "# revol_util\n",
        "# new_df['revol_util:<44'] = np.where((preprocess_data['revol_util'] <=44), 1,0)\n",
        "# new_df['revol_util:44-89'] =np.where((preprocess_data['revol_util'] > 44) & (preprocess_data['revol_util'] <= 89),1, 0)\n",
        "# new_df['revol_util:>89'] = np.where((preprocess_data['revol_util'] >89), 1,0)\n",
        "\n",
        "# tot_cur_balance\n",
        "new_df['tot_cur_bal:<40000'] = np.where((preprocess_data['tot_cur_bal'] <= 40000), 1, 0)\n",
        "new_df['tot_cur_bal:40000-80000'] = np.where((preprocess_data['tot_cur_bal'] > 40000) & (preprocess_data['tot_cur_bal'] <= 80000), 1, 0)\n",
        "new_df['tot_cur_bal:80000-120000'] = np.where((preprocess_data['tot_cur_bal'] > 120000) & (preprocess_data['tot_cur_bal'] <= 160000), 1, 0)\n",
        "new_df['tot_cur_bal:120000-160000'] = np.where((preprocess_data['tot_cur_bal'] > 120000) & (preprocess_data['tot_cur_bal'] <= 160000), 1, 0)\n",
        "new_df['tot_cur_bal:160000-200000'] = np.where((preprocess_data['tot_cur_bal'] > 160000) & (preprocess_data['tot_cur_bal'] <= 200000), 1, 0)\n",
        "new_df['tot_cur_bal:200000-240000'] = np.where((preprocess_data['tot_cur_bal'] > 200000) & (preprocess_data['tot_cur_bal'] <= 240000), 1, 0)\n",
        "new_df['tot_cur_bal:240000-320000'] = np.where((preprocess_data['tot_cur_bal'] > 240000) & (preprocess_data['tot_cur_bal'] <= 320000), 1, 0)\n",
        "new_df['tot_cur_bal:320000-400000'] = np.where((preprocess_data['tot_cur_bal'] > 320000) & (preprocess_data['tot_cur_bal'] <= 400000), 1, 0)\n",
        "new_df['tot_cur_bal:>400000'] = np.where((preprocess_data['tot_cur_bal'] > 400000), 1, 0)\n",
        "\n",
        "# mths_since_last_credit_pull_d\n",
        "new_df['mths_since_last_credit_pull_d:<65'] = np.where((preprocess_data['mths_since_last_credit_pull_d']<=65), 1,0)\n",
        "new_df['mths_since_last_credit_pull_d:65-76'] = np.where((preprocess_data['mths_since_last_credit_pull_d']>65)&(preprocess_data['mths_since_last_credit_pull_d']<=76),1,0)\n",
        "new_df['mths_since_last_credit_pull_d:>76'] = np.where((preprocess_data['mths_since_last_credit_pull_d']>76), 1,0)\n",
        "\n",
        "# mths_since_issue_d_factor\n",
        "new_df['mths_since_issue_d_:<70.8'] = np.where((preprocess_data['mths_since_issue_d']<=70.8), 1,0)\n",
        "new_df['mths_since_issue_d_:>70.8-73.6'] = np.where((preprocess_data['mths_since_issue_d'] >70.8) & (preprocess_data['mths_since_issue_d']<=73.6), 1,0)\n",
        "new_df['mths_since_issue_d_:73.6-76.4'] = np.where((preprocess_data['mths_since_issue_d']>70.8) & (preprocess_data['mths_since_issue_d']<=76.4), 1,0)\n",
        "new_df['mths_since_issue_d_:>76.4-79.2'] = np.where((preprocess_data['mths_since_issue_d'] >76.4) & (preprocess_data['mths_since_issue_d']<=79.2), 1,0)\n",
        "new_df['mths_since_issue_d_:>79.2-82'] = np.where((preprocess_data['mths_since_issue_d'] >79.2) & (preprocess_data['mths_since_issue_d']<=82), 1,0)\n",
        "new_df['mths_since_issue_d_>82-84'] = np.where((preprocess_data['mths_since_issue_d'] >82) & (preprocess_data['mths_since_issue_d']<=84), 1,0)\n",
        "new_df['mths_since_issue_d_:>84-90.4'] = np.where((preprocess_data['mths_since_issue_d'] >84) & (preprocess_data['mths_since_issue_d']<=90.4), 1,0)\n",
        "new_df['mths_since_issue_d_:>90.4-96'] = np.where((preprocess_data['mths_since_issue_d'] >90.4) & (preprocess_data['mths_since_issue_d']<=96), 1,0)\n",
        "\n",
        "new_df['out_prncp:<3000'] = np.where((preprocess_data['out_prncp']<=3000), 1,0)\n",
        "new_df['out_prncp:3000-6000'] = np.where((preprocess_data['out_prncp']>3000)&(preprocess_data['out_prncp']<=6000), 1,0)\n",
        "new_df['out_prncp:6000-10000'] = np.where((preprocess_data['out_prncp']>6000)&(preprocess_data['out_prncp']<=10000), 1,0)\n",
        "new_df['out_prncp:10000-12000'] = np.where((preprocess_data['out_prncp']>10000)&(preprocess_data['out_prncp']<=12000), 1,0)\n",
        "new_df['out_prncp:>12000'] = np.where((preprocess_data['out_prncp']>12000), 1,0)\n",
        "\n",
        "\n",
        "\n",
        "new_df['good_bad'] = preprocess_data.loc[:, 'good_bad']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0x8w6TdIqwmN"
      },
      "outputs": [],
      "source": [
        "# Display first 10 rows of new_df\n",
        "pd.options.display.max_columns = None\n",
        "new_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhL7PLG3qwmO"
      },
      "outputs": [],
      "source": [
        "new_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "480m3H_SqwmP"
      },
      "outputs": [],
      "source": [
        "new_df1 = new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoaNrVRLqwmP"
      },
      "source": [
        "Remove one dummy variable for each original variable, otherwise we will go into the dummy variable trap. The dummy variables with the lowest WOE will be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j7-tAU3qwmR"
      },
      "outputs": [],
      "source": [
        "# Dummy categories dropped\n",
        "ref_categories = ['home_ownership:OTHER_NONE_RENT_ANY', 'total_rec_int:<1000', 'total_pymnt:<5000','total_rev_hi_lim:<10000', 'grade:G', 'verification_status:VERIFIED', 'purpose:SMALL_BUSINESS_EDUCATIONAL_RENEWABLE_ENERGY_MOVING',\n",
        "                 'addr_state:NE_IA_NV_HI_FL_AL', 'initial_list_status:F', 'term:60', 'mths_since_issue_d_:>90.4-96','int_rate:21.932-26.06', 'dti:27-40',\n",
        "                  'annual_inc:<32000', 'inq_last_6mths:4-7', 'tot_cur_bal:<40000', 'mths_since_last_credit_pull_d:>76', 'out_prncp:>12000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IATyuCTSqwmY"
      },
      "outputs": [],
      "source": [
        "new_df.drop(columns=ref_categories, inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmXNk888qwmZ"
      },
      "outputs": [],
      "source": [
        "new_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3EsjHpiqwmb"
      },
      "outputs": [],
      "source": [
        "# Check the class labels are balanced\n",
        "\n",
        "from yellowbrick.target import ClassBalance\n",
        "X= new_df.drop(columns='good_bad', axis=1)\n",
        "y = new_df['good_bad']\n",
        "visualizer = ClassBalance()\n",
        "visualizer.fit(y)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRk84n_cqwmd"
      },
      "source": [
        "Based on graph we see that individuals who are classified as a bad borrowers have very few observations. This class imbalance can affect our model when it turns to training test. To solve this problem we will oversample the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLYqZ5x7qwmf"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_13jTgu5qwmg"
      },
      "outputs": [],
      "source": [
        "# Check imbalance data for training dataset\n",
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries for Train Test Model"
      ],
      "metadata": {
        "id": "m3L4Px-dZWNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j70s2fXTqwml"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.combine import SMOTETomek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCm_2lYEqwmn"
      },
      "outputs": [],
      "source": [
        "# Deal with imbalance data\n",
        "os = RandomOverSampler()\n",
        "X_train_o, y_train_o = os.fit_resample(X_train, y_train)\n",
        "y_train_series = pd.Series(y_train_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YozoKEeIqwmo"
      },
      "outputs": [],
      "source": [
        "# Check value counts after oversampling\n",
        "y_train_series.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Obov9muqwmp"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_o, y_train_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hovP1biPqwmq"
      },
      "outputs": [],
      "source": [
        "# Predict the model\n",
        "y_preds = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwCNOVjGqwmr"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test, y_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBIdIRkyqwms"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba = model.predict_proba(X_test)\n",
        "y_hat_test_proba = y_hat_test_proba[:][: , 1]\n",
        "y_test_temp = y_test.copy()\n",
        "y_test_temp.reset_index(drop = True, inplace = True)\n",
        "y_test_proba = pd.concat([y_test_temp, pd.DataFrame(y_hat_test_proba), pd.DataFrame(y_preds)], axis = 1)\n",
        "y_test_proba.columns = ['y_test_class_actual', 'y_hat_test_proba', 'y_hat_test']\n",
        "y_test_proba.index = X_test.index\n",
        "y_test_proba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04e8Vm6-qwmu"
      },
      "outputs": [],
      "source": [
        "# Extract the values required to plot a ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr)\n",
        "# Plot a secondary diagonal line, to show randomness of model\n",
        "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g5oA_2Jqwmv"
      },
      "outputs": [],
      "source": [
        "# Area under receiver operating charateristic curve (AUROC)\n",
        "AUROC = roc_auc_score(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])\n",
        "AUROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi4TCAq6qwmx"
      },
      "outputs": [],
      "source": [
        "Gini = AUROC * 2 - 1\n",
        "Gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S0RPIG5qwmy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "# Plot a PR curve\n",
        "# Calculate no_skill line as the proportion of the positive class\n",
        "no_skill = len(y_test[y_test == 1]) / len(y)\n",
        "# {lot the no_skill precision-recall curve\n",
        "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "\n",
        "# Calculate inputs for the PR curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])\n",
        "# Plot PR curve\n",
        "plt.plot(recall, precision, marker='.', label='Logistic')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.title('PR curve');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDhbe6RSqwmz"
      },
      "outputs": [],
      "source": [
        "# Precision recall score\n",
        "auc_pr = auc(recall, precision)\n",
        "auc_pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6IgubDUqwm0"
      },
      "outputs": [],
      "source": [
        "# Calculate ks statistic\n",
        "actual_predicted_probs_df = y_test_proba.sort_values('y_hat_test_proba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT8Ahoq4qwm1"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MSuPSYhqwm2"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfRICPVNqwm3"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df = actual_predicted_probs_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAUHZODuqwm4"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df['cum_n_pop'] = actual_predicted_probs_df.index +1\n",
        "actual_predicted_probs_df['cum_good'] = actual_predicted_probs_df['y_test_class_actual'].cumsum()\n",
        "actual_predicted_probs_df['cum_bad'] = actual_predicted_probs_df['cum_n_pop'] - actual_predicted_probs_df['y_test_class_actual'].cumsum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXHi6Iudqwm4"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELc7I--mqwm5"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df['cum_n_%'] = actual_predicted_probs_df['cum_n_pop']/(actual_predicted_probs_df.shape[0])\n",
        "actual_predicted_probs_df['cum_good_%'] = actual_predicted_probs_df['cum_good']/actual_predicted_probs_df['y_test_class_actual'].sum()\n",
        "actual_predicted_probs_df['cum_bad_%'] = actual_predicted_probs_df['cum_bad']/ (actual_predicted_probs_df.shape[0]-actual_predicted_probs_df['y_test_class_actual'].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZxKr8e3qwm7"
      },
      "outputs": [],
      "source": [
        "actual_predicted_probs_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odRzVBYdqwm9"
      },
      "outputs": [],
      "source": [
        "plt.plot(actual_predicted_probs_df['cum_n_%'], actual_predicted_probs_df['cum_bad_%'])\n",
        "plt.plot(actual_predicted_probs_df['cum_n_%'], actual_predicted_probs_df['cum_n_%'], linestyle='--', c='k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piVYf5EBqwm_"
      },
      "outputs": [],
      "source": [
        "plt.plot(actual_predicted_probs_df['y_hat_test_proba'], actual_predicted_probs_df['cum_bad_%'], c='r')\n",
        "plt.plot(actual_predicted_probs_df['y_hat_test_proba'], actual_predicted_probs_df['cum_good_%'], c='g')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fagGKDnqwnA"
      },
      "outputs": [],
      "source": [
        "ks = max(actual_predicted_probs_df['cum_bad_%'] - actual_predicted_probs_df['cum_good_%'])\n",
        "print('The KS score is ',ks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Deployment**"
      ],
      "metadata": {
        "id": "CEp3fjt5bKCN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9NwYq_9qwnC"
      },
      "outputs": [],
      "source": [
        "# Save the model \n",
        "import pickle\n",
        "filename = 'credit_risk_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Laila Awalia Saummi_VIX_ID/X Partners.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XDQuVuNtqwlV"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}